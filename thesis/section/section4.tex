\section{Implementation\label{cha:chapter4}}
\noindent This chapter discusses the implementation aspects of web crawler. It explains the
choice of programming languages, development tools used to implement the crawler engine. Furthermore,emphasis is given on explaining the internal architecture behind the crawler engine,along with the details of functionality performed by each module. Focus is also given on discussing the overall work flow of the crawler engine.\\ 
\subsection{Choice of programming language}
\noindent Before going for implementation, it is important to know that the whole process involve two major parts. In the first step, identification of hyper giants will be performed using different features like number of links, number of IP addresses, BGP prefixes etc. In the second step the dependency between hyper giants and popular websites will be discussed based on number of web objects delivered from hyper giants to popular websites. Hence a programming language which is powerful in development as well as data analysis will be chosen. So that any kind of dependency between two steps can be handled easily. Again language should be free, open and usable by anyone who wishes to run this crawler implementation in future. Moreover the language should have good number of open libraries, big community and should have a lot of online documentation. For this purpose python language is chosen for the implementation as it fulfills all necessary requirement. For data analysis pyspark (Python version of spark) will be used.\\

\subsection{Development Tools}
\noindent For development, it is important to choose correct IDE which allows to write code in less time with minimum effort.Along with this it helps in code completion, syntax highlighting, refactoring. For this purpose "sublime editor" which is free and easy to write python code is chosen.\\

\noindent Now second most important aspect is to choose python web crawling tool which will suit the requirement and can be used in future. There are couple of famous web crawling tools available like urlib, beautiful soup, scrapy etc. But Python Scrapy is the best out there. Scrapy crawling is faster than any other platforms, since it uses asynchronous operations (on top of Twisted). Scrapy has better and faster support for parsing (x)html on top of libxml2. Scrapy is a mature framework with full unicode, redirection handling, gzipped responses, odd encodings, integrated HTTP cache etc. Again it is open source having a big community and documentation.\\

\noindent There are lot of different machine learning tools available but as it is expected that resultant data will be order of some gigabytes, it is better to use that tool which can process data faster.Python and R are popular languages for data analysis due to the large number of modules or packages that are readily available. But traditional uses of these tools are often limiting, as they process data on a single machine where the movement of data becomes time consuming, the analysis requires sampling (which often does not accurately represent the data), and moving from development to production environments requires extensive re-engineering. Spark provides a powerful, unified engine that is both fast and easy to use. Hence Pyspark will be used for initial data analysis and further analysis will be carried using python and RStudio.\\

\noindent Apart from above tools, some other tools used for the implementation. The following are the list of important softwares and tools used.

\begin{itemize}
  \item dnspython:it is a DNS toolkit for python.This is used in the code to get the A records of hosts.
  \item pygeoip :The libarary is used to get the ASN numbers associated with IP addresses.This library is based on Maxmind’s GeoIP C API.
  \item urlparse :this is used to convert a relative url to an absolute url.
  \item Public suffix List :This is the collection of all registered host names given by all internet users.The Public Suffix List is an initiative of Mozilla, but is maintained as a community resource.It is available for use in any software, but was originally created to meet the needs of browser manufacturers.In our code we use it to get second level domain from a host name.The "effective\_tld\_names\. dat" is the file which is free downloadable from their site.
  \item IPython :IPython notebook is used for pyspark code writing and execution.
  \item matplotlib :Python library used to for making different graphs used in this thesis.
  \item pygal :Pygal is also another python library which we used to make graphs.
\end{itemize}

\subsection{Design of Crawler Engine}
\noindent In this section,the internal architecture behind the crawler engine will be discussed.The design process involves two parts,first one is the crawler engine which takes 100,000 top ranked websites of Alexa as input,crawl the websites, return the result file as output from the engine and second part is processing of result file using data processing engine which internally use "pyspark" to clean up the crawled data.\\

\subsubsection{Crawler Engine}
\noindent In this section, the internal architecture of the crawler engine will be discussed. The main objective of crawler engine is to take input data in the format of text file which contain the top 100,000 top ranked websites of Alexa. Then divide this master domain list into multiple sub domain lists with equal weighted websites and process each sub domain list using separate crawler instances. Each crawler instance work independently and store all website information like HTTP header information, DNS resolution analysis etc. in result file. The core of this cralwer engine is "Scrapy" which is used for crawling the website links. In the following sections the architecture of scrapy framework, the main data types of scrapy framework used for implementation will be discussed. In the subsequent section, the overall work flow of the crawler engine will be discussed.\\
\subsubsection{Scrapy Framework}
\noindent Scrapy framework is one of top ranked open source project, a fast web crawling framework, used to crawl websites and extract structured data from their pages. It provide option for both focused and broad crawling.In case of focused crawler scrapy crawls a specific domain while in case of broad crawling a large (potentially unlimited) number of domains can be crawled. Hence for the implementation broad crawling will be used. As can be seen from Figure \ref{fig:scrapy} [20],the main components of the framework contain scrapy engine, scheduler, downloaders, spiders, item pipeline, downloader middlewares.\\

\begin{itemize}
\item Scrapy Engine :
The engine is responsible for controlling the data flow between all components of the system, and triggering events when certain actions occur. 

\item Scheduler :
The Scheduler receives requests from the engine and enqueues them for feeding them later (also to the engine) when the engine requests them.

\item Downloader :
The Downloader is responsible for fetching web pages and feeding them to the engine which, in turn, feeds them to the spiders.

\item Spiders :
Spiders use for to parse responses and extract items from them or additional URLs (requests) to follow.

\item Item Pipeline :
The Item Pipeline is responsible for processing the items once they have been extracted (or scraped) by the spiders. Typical tasks include cleansing, validation and persistence (like storing the item in a database). For more information see Item Pipeline.

\item Downloader middlewares :
Downloader middlewares are specific hooks that sit between the Engine and the Downloader and process requests when they pass from the Engine to the Downloader, and responses that pass from Downloader to the Engine.

\end{itemize}

\begin{figure}[htb]
  \centering
  \includegraphics[width=14cm]{/home/sakib/soumya/latex/scrapy_architecture}\\
  \caption{Scrapy Architecture}
  \label{fig:scrapy}
\end{figure}

\subsubsection{Scrapy data types}
\noindent Scrapy uses some specific classes and strings which are used to crawl data. These are written in python and are open source. Scrapy also use twisted framework for multi threading. Some of the classes which are going to be used for implementation, are as follows,\\

\begin{enumerate}
   \item Spider :Spiders are the classes used to crawl single or multiple domains using different methods like start\_requests(),parse().
   \begin{itemize}
     \item start\_requests() :This scrapy method is used to pass domains to parse method for further crawling process.
     \item parse() :he parse method is in charge of processing the response and returning scraped data and/or more URLs to follow. Other Requests callbacks have the same requirements as the Spider class.
   \end{itemize}
      \item Twisted Framework:Twisted supports an abstraction over raw threads — using a thread as a deferred source. Thus, a deferred is returned immediately, which will receive a value when the thread finishes. Callbacks can be attached which will run in the main thread, thus alleviating the need for complex locking solutions. A prime example of such usage, which comes from Twisted's support libraries, is using this model to call into databases. The database call itself happens on a foreign thread, but the analysis of the result happens in the main thread.
      \item Requests objects:Typically, Request objects are generated in the spiders and pass across the system until they reach the Downloader, which executes the request.A Request object represents an HTTP request, which is usually generated in the Spider and executed by the Downloader, and thus generating a Response.
	\item Response objects :Request object returns a Response object which travels back to the spider that issued the request.A Response object represents an HTTP response, which is usually downloaded (by the Downloader) and fed to the Spiders for processing.

\end{enumerate}
\subsection{Work flow}
\noindent In this section the overall work flow of the crawler engine will be discussed. The crawler engine is used to perform multiple operations. It take input data in the format of text file which contain the top 100,000 top ranked websites of Alexa. Then divide this master domain list into multiple sub domain lists with equal weighted websites and process each sub domain list using separate crawler instances. Each crawler instance work independently and store all website information like HTTP header information, DNS resolution analysis etc. in result file. The core of this cralwer engine is "Scrapy" which is used for crawling the website links. The main focus is to create multiple instances of crawler engine. As scrapy is a memory greedy tool,after intensive testing it is decided to create 20 parallel threads which work independently. Each instance of the crawler will take separate input file which contain 5000 domains. Master domain list should be divided properly so that all the crawlers will complete their crawling in almost same time. Hence multiple sublists with equal weight will be created where weight refers to the rank of the websites provided by Alexa. The master domain list contain the website links and the rank of the websites given by Alexa. So after division the first sublist will contain the websites of 1st rank, 21st rank, ..., 99981st rank, second list will contain 2nd rank, 22nd rank, ..., 99982nd rank websites and so forth for other instances.After the division each sublist will contain 5000 websites with nearly equal weight of website ranks. Each sublist will be sent as input to multiple instances of crawler shown in Figure \ref{fig:crawl}. \\

\begin{figure}[htb]
  \centering
  \includegraphics[width=14cm]{/home/sakib/soumya/latex/archi-1}\\
  \caption{Pre Crawling Step}
  \label{fig:crawl}
\end{figure}

\noindent In the subsequent step each crawler instance will take 5000 domains and parse one by one domain. For each URL crawler engine downloads the corresponding web page, extract the linked URLs, and check each url to see whether the extracted url is a fresh url which has not already been seen . With this architecture a very large number of independent crawls of the white listed domains obtained from Alexa can be crawled. Along with this, the crawler engine also extract HTTP header information, ARecord details ,ASN details.\\

\subsection{Conclusion}
\noindent This section discusses the softwares used to implement the required functionality. The motivation behind choosing different languages, software tools used during the implementation are discussed. The internal architecture of the crawler engine used for crawling 100,000 top ranked websites is also explained.\\
\clearpage